<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Keything的日志</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Keything的日志">
<meta property="og:url" content="http://keything.github.io/index.html">
<meta property="og:site_name" content="Keything的日志">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Keything的日志">
  
    <link rel="alternate" href="/atom.xml" title="Keything的日志" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Keything的日志</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://keything.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Spark-DataSource-Hive Tables" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/11/16/Spark-DataSource-Hive Tables/" class="article-date">
  <time datetime="2021-11-16T09:45:21.249Z" itemprop="datePublished">2021-11-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://keything.github.io/2021/11/16/Spark-DataSource-Hive Tables/" data-id="ckw1x701z00005enumzuqbp7y" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-分布式-事务" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/11/14/分布式-事务/" class="article-date">
  <time datetime="2021-11-14T02:34:03.000Z" itemprop="datePublished">2021-11-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/11/14/分布式-事务/">分布式-事务</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文是《数据密集型应用系统设计》（英文：Designing Data-Intensive Applications) 第7章学习总结。</p>
<p>在总结之前，提问几个问题：</p>
<ol>
<li>什么是事务?</li>
<li>为什么引入事务</li>
<li>事务中最核心的问题是什么？</li>
<li>事务中隔离级别有哪些级别，级别划分依据是什么？</li>
<li>隔离级别解决了哪些问题，哪些没有解决</li>
<li>如何实现这些隔离级别</li>
</ol>
<h2 id="一、事务"><a href="#一、事务" class="headerlink" title="一、事务"></a>一、事务</h2><h3 id="什么是事务"><a href="#什么是事务" class="headerlink" title="什么是事务"></a>什么是事务</h3><p>在应用程序中，将一组数据库的读写组成一个逻辑操作单元；即事务中所有读写是一个执行的整体，整个事务要么成功（提交），要么失败（中止或回滚）。如果失败，应用程序可以安全地重试。</p>
<h3 id="为什么引入事务"><a href="#为什么引入事务" class="headerlink" title="为什么引入事务"></a>为什么引入事务</h3><p>简化应用层的编程模型：当一组读写中部分写入成功，部分写入失败时，我们需要将成功的进行回滚；如果数据库不引入事务，就需要业务层自己处理。 </p>
<p>如何判断是否需要事务？</p>
<blockquote>
<p>我们需要确切地理解事务能够提供哪些安全性保证，背后的代价又是什么。 </p>
</blockquote>
<h3 id="事务提供的安全性保证"><a href="#事务提供的安全性保证" class="headerlink" title="事务提供的安全性保证"></a>事务提供的安全性保证</h3><p>事务提供的安全性保证即大家熟悉的ACID。</p>
<ul>
<li>Atomic（原子性）：执行要么全部成功，要么全部失败。在出错时中止事务，并将部分完成的写入全部丢弃。 </li>
<li>Consistency(一致性）：这儿的一致性是符合数据的约束条件（比如数据x=y,x+y=100等)</li>
<li>Isolation（隔离）：意味着并发执行的多个事务相互隔离，他们不能互相交叉。经典数据库教材中把隔离定义为可串行化。</li>
<li>Duration（持久化）：数据持久化</li>
</ul>
<p>其中AID是数据库自身属性，C是应用层属性，AID来保证C。</p>
<h2 id="二、隔离级别"><a href="#二、隔离级别" class="headerlink" title="二、隔离级别"></a>二、隔离级别</h2><h3 id="2-1-什么时候需要隔离？"><a href="#2-1-什么时候需要隔离？" class="headerlink" title="2.1 什么时候需要隔离？"></a>2.1 什么时候需要隔离？</h3><p>如果两个事务操作的是不同的数据，即不存在数据依赖关系，那么就可以安全地并行。</p>
<p>只有出现某个事务修改数据而另外一个事务同时读取该数据，或者两个事务同时修改相同数据时，才会引起并发问题。</p>
<h3 id="2-2-隔离级别的定义"><a href="#2-2-隔离级别的定义" class="headerlink" title="2.2 隔离级别的定义"></a>2.2 隔离级别的定义</h3><p>ANSI/ISO SQL-92中定义了四种隔离级别Read-Commited, Repeatable Read, Snapshot Isolation, Seriable。这些隔离级别是通过经典的序列化定义和三种被禁止的子序列来定义的。三种被禁止的子序列是Dirty Read, Non-repeatable Read 和 phantom(幻象）。</p>
<blockquote>
<p>通俗来讲，就是禁止了哪些问题就达到了某个隔离级别；隔离级别也都是与特定的锁有对应关系的</p>
</blockquote>
<p>隔离级别也是与lock有关的。</p>
<h3 id="2-3-异常现象（异常子序列）"><a href="#2-3-异常现象（异常子序列）" class="headerlink" title="2.3 异常现象（异常子序列）"></a>2.3 异常现象（异常子序列）</h3><p>这些异常现象将会用如下格式进行详细描述：</p>
<ul>
<li>问题的文字描述</li>
<li>问题的序列化表示</li>
<li>问题的例子</li>
<li>问题的解决方案</li>
</ul>
<p>关于锁的解释</p>
<ol>
<li>long-duration vs short duration<ul>
<li>长期锁是在加锁以后，直到事务结束或回滚才释放锁</li>
<li>短期锁是在动作结束以后，就立即释放锁</li>
</ul>
</li>
<li><p>predict  vs 行锁</p>
<ul>
<li>predict lock是针对一个查询条件加锁</li>
<li>行锁是针对特定一行记录加锁</li>
</ul>
</li>
</ol>
<h4 id="脏写-Dirty-Write-P0"><a href="#脏写-Dirty-Write-P0" class="headerlink" title="脏写 Dirty Write P0"></a>脏写 Dirty Write P0</h4><ol>
<li>问题的文字描述：一个正在进行的事务覆盖了另外一个事务尚未提交的写入。</li>
<li>问题的序列化表示：W1(x)…W2(x) … (c1 or a1)</li>
<li>问题的解决办法：对写入加一个long-duration write lock。</li>
<li>说明：<ul>
<li>Dirty write是ANSI/ISO SQL-92中没有提到的，但是需要避免，是基础。</li>
<li>如果有脏写，那么会没有办法回滚，也可能影响数据约束（x=y or x+y=100)</li>
</ul>
</li>
<li>举例：Suppose T1 writes x=y=1 and T2 writes x=y=2, the following history violates the integrity constraint.</li>
</ol>
<p><img src="https://blog.acolyer.org/wp-content/uploads/2016/02/dirty-write.png" alt=""></p>
<h4 id="脏读-Dirty-Read-P1"><a href="#脏读-Dirty-Read-P1" class="headerlink" title="脏读 Dirty Read P1"></a>脏读 Dirty Read P1</h4><ol>
<li>问题的文字描述：一个正在进行的事务读取了另外一个事务未提交的写入。</li>
<li>问题的序列化表示：W1(x)..R2(x) ..(c1 or a1)</li>
<li>问题的解决办法：加入一个short-duration read lock。<ul>
<li>写是long-duration write lock, 读是short-duration read lock，当正在发生写入的事务占有锁时，读取的事务因为没有办法获得读锁，只能等待。 </li>
<li>注明：<a href="http://arxiv.org/pdf/cs/0701157.pdf" target="_blank" rel="noopener">1</a>中使用读锁来实现，但是在最新的数据库中数据库维护新旧两个取值，事务提交之前读取旧值；仅当写事务提交以后，才会切换到读取新值。</li>
</ul>
</li>
<li>说明：<ul>
<li>解决该问题的隔离级别就是 Read-Commited（读-提交） 隔离级别</li>
</ul>
</li>
<li>举例：<ul>
<li>序列化：H1: r1[x=50]w1[x=10]r2[x=10]r2[y=50]c2 r1[y=50]w1[y=90]c1</li>
<li>如下图所示 t2中 x+y=60，其中x=10是脏读，<img src="//keything.github.io/2021/11/14/分布式-事务/dirty read.jpeg" alt=""></li>
</ul>
</li>
</ol>
<h4 id="不可重复读-unrepeatable-read-P2"><a href="#不可重复读-unrepeatable-read-P2" class="headerlink" title="不可重复读 unrepeatable-read P2"></a>不可重复读 unrepeatable-read P2</h4><ol>
<li>问题的文字描述：事务在不同的时间点看到不同值。事务T2修改了之前事务T1读过的数据，不管T1、T2是提交还是回滚，就认为是nonrepeatable-read</li>
<li>问题的序列化表示：R1(x)..W2(x)..(c1 or a1)</li>
<li>问题的解决办法：snapshot isolation，多版本</li>
<li>说明：不可重复读实际是 读倾斜的x等于y的一个特例</li>
<li>举例：因为不可重复读可以看做是读倾斜x等于y的一个特例，可以去看 读倾斜的例子。</li>
</ol>
<h4 id="幻象-phantom-P3"><a href="#幻象-phantom-P3" class="headerlink" title="幻象 phantom P3"></a>幻象 phantom P3</h4><ol>
<li><p>问题的文字描述：</p>
<p> 事务T1读取一组数据集合满足条件<search condition="">。事务T2创建了满足T1中<search condition="">的数据集合并提交，那么T1重复读取<search condition="">时，将会获取到跟之前不同的数据</search></search></search></p>
</li>
<li><p>问题的序列化表示：R1(P)… W2(y in P)…(c1 or a1)</p>
</li>
<li>问题的解决办法：采用区间范围锁 index-range lock，又叫next-key lock</li>
<li>说明：<ul>
<li>Nonrepeatable和幻象的区别 一个单个对象，一个是多个对象</li>
</ul>
</li>
</ol>
<h5 id="严格意义的幻象-A3"><a href="#严格意义的幻象-A3" class="headerlink" title="严格意义的幻象 A3"></a>严格意义的幻象 A3</h5><ol>
<li>问题的序列化表示：R1(p)..W2(y in p)..c2..r1(p)…c1</li>
<li>与P3相比更加严格，有一次T1的读取操作。</li>
<li>问题解决办法：使用snapshot isolation即可解决</li>
</ol>
<h4 id="更新丢失-Losst-update-P4"><a href="#更新丢失-Losst-update-P4" class="headerlink" title="更新丢失 Losst update P4"></a>更新丢失 Losst update P4</h4><ol>
<li>问题的文字描述：事务T2对X的修改被事务T1的修改覆盖。之后事务T1提交，从外界看来，T1对X的修改丢失</li>
<li>问题的序列化表示：R1(x)..W2(x)..W1(x)..C1</li>
<li>问题的解决办法：snapshot isolation，多版本</li>
<li>举例：<ul>
<li>序列化是 H4: r1[x=100]r2[x=100]w2[x=120 c2 w1 [x=130] c1</li>
<li>预期是从100经过+20，+30，最后取值是150；实际是130。如图所示 <img src="//keything.github.io/2021/11/14/分布式-事务/lost update.jpeg" alt=""></li>
</ul>
</li>
</ol>
<h4 id="更新丢失-Cursor-Lost-update-P4C"><a href="#更新丢失-Cursor-Lost-update-P4C" class="headerlink" title="更新丢失 Cursor Lost update P4C"></a>更新丢失 Cursor Lost update P4C</h4><p>P4C is a variation of the Lost Update phenomenon that involves a SQL cursor. In the history below, let rc(x) represent a read of the data item x under the cursor, and wc(x) a write of the data item x under the cursor. If we allow another transaction T2 to write to x in between the read-cursor and write-cursor actions of T1, then its update will be lost.</p>
<p>序列化表示：P4C: rc1[x]..w2[x] ..w1[x] ..c1<br><img src="https://blog.acolyer.org/wp-content/uploads/2016/02/cursor-lost-update.png" alt=""></p>
<h3 id="数据不一致-data-item-constraint-violation-A5"><a href="#数据不一致-data-item-constraint-violation-A5" class="headerlink" title="数据不一致 data item constraint violation A5"></a>数据不一致 data item constraint violation A5</h3><ol>
<li>问题的文字描述：两个数据X和Y满足某些限制，可能有以下异常情况出现：<ul>
<li>A5A Read Skew: 假设T1读取X，之后T2更新x和y到了新的取值，并提交；之后T1读取y，则x和y的限制被打破。</li>
<li>A5B Write Skew: 假设T1读取X和Y，之后T2读取X和Y，并写入X，然后提交；之后T1 写入Y，那么存在X和Y的限制被打破的可能</li>
</ul>
</li>
</ol>
<h4 id="A5A-Read-Skew"><a href="#A5A-Read-Skew" class="headerlink" title="A5A Read Skew:"></a>A5A Read Skew:</h4><ol>
<li>序列化表示：A5A: R1(x)..W2(x)..W2(y)..C2 …R1(y) .. (c1 or a1)</li>
<li>举例：<pre><code>以银行转账为例，初始化x=y=50，从x转走40到y，最终预期是x=10,y=90。
出现脏读时，其读取数据是 r1[x=5]r2[x=50]w2[x=10]r2[y=50]w2[y=90]c2r1[y=90]c1
数据如图所示，t1中x+y=140不满足100的限制。 ![](分布式-事务/repeatable read.jpeg)
</code></pre></li>
</ol>
<h4 id="A5B-Write-Skew"><a href="#A5B-Write-Skew" class="headerlink" title="A5B Write Skew:"></a>A5B Write Skew:</h4><ol>
<li>序列化表示：A5B：R1(x)..R2(y)..W1(y)..W2(x)..(c1 and c2 occur)</li>
<li>举例：</li>
</ol>
<p><img src="https://blog.acolyer.org/wp-content/uploads/2016/02/write-skew1.png" alt=""></p>
<h3 id="2-4-隔离级别"><a href="#2-4-隔离级别" class="headerlink" title="2.4 隔离级别"></a>2.4 隔离级别</h3><p>可以通过刻画他们禁止的异常情况来刻画隔离级别</p>
<p><img src="//keything.github.io/2021/11/14/分布式-事务/isolation types.jpeg" alt=""></p>
<p>他们之间的关系如下图所示 <img src="//keything.github.io/2021/11/14/分布式-事务/isolation levels and their relationship.jpeg" alt=""></p>
<p>我们可以通过他们允许的非序列化历史来比较隔离级别：</p>
<ul>
<li>L1 is weaker than L2 if L1 permits non-serializable histories that L2 does not, and every non-serializable history under L2 is also a non-serializable history under L1. We write L1 &lt;&lt; L2.</li>
<li>L1 and L2 are equivalent if the sets of non-serializable histories permitted by them both are identical. We write L1 == L2</li>
<li>L1 and L2 may also be incomparable. If L1 permits a non-serializable history that L2 does not, and vice-versa, then L1 is not weaker than L2, but L2 is also not weaker than L1. We write L1 &lt;&gt; L2.</li>
</ul>
<h4 id="结论1"><a href="#结论1" class="headerlink" title="结论1:"></a>结论1:</h4><p>我们可以得到 </p>
<blockquote>
<p>Degree 0 (everything goes) &lt;&lt; Read Uncommitted &lt;&lt; Read Committed &lt;&lt; Cursor Stability &lt;&lt; Repeatable Read &lt;&lt; Serializable.</p>
</blockquote>
<p>重点解释一下 Cursor Stability，Cursor Stability是扩展Read Commited锁行为。//TODO 待补充</p>
<h4 id="结论2："><a href="#结论2：" class="headerlink" title="结论2："></a>结论2：</h4><blockquote>
<p>Read commited &lt;&lt; snapshot isolation</p>
</blockquote>
<blockquote>
<p>ANOMALY Serializable &lt;&lt; Snapshot isolation</p>
</blockquote>
<blockquote>
<p>repeatable read &lt;&gt;  snapshot isolation 这两个是不可比较的。</p>
</blockquote>
<p>许多应用通过使用cursor stability 或 oracle’s read consistency isolation 来避免锁竞争。对于这些应用而言，使用Snapshot Isolation会更好：</p>
<ul>
<li>避免lost update</li>
<li>严格意义的幻象(如上面所说明的A3，但不能定义更广的P3）</li>
<li>从不阻塞只读的事务，读取不会阻塞更新</li>
</ul>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ol>
<li>Hal Berenson, Philip A. Bernstein, Jim N. Gray, et al.: “<a href="http://research.microsoft.com/pubs/69541/tr-95-51.pdf" target="_blank" rel="noopener">A Critique of ANSI SQL Isolation Levels,</a>” at ACM International Conference on Management of Data (SIG‐ MOD), May 1995</li>
<li><a href="https://blog.acolyer.org/2016/02/24/a-critique-of-ansi-sql-isolation-levels/" target="_blank" rel="noopener">A Critique of ANSI SQL Isolation Levels 解释</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/187597966" target="_blank" rel="noopener">A Critique of ANSI SQL Isolation Levels 阅读笔记</a></li>
<li>数据密集型应用系统设计 chapter 7 事务</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://keything.github.io/2021/11/14/分布式-事务/" data-id="ckw1x7059005a5enuw1k1ob4a" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式/">分布式</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-golang继承" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/04/29/golang继承/" class="article-date">
  <time datetime="2019-04-29T02:52:38.000Z" itemprop="datePublished">2019-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/04/29/golang继承/">golang继承</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>项目中已经经常使用golang继承，现在总结一下，主要摘在<a href="https://hackthology.com/golangzhong-de-mian-xiang-dui-xiang-ji-cheng.html" target="_blank" rel="noopener">Golang中的面向对象继承</a></p>
<p>总结如下：</p>
<ol>
<li>golang使用组合，可以将两个结构体简单组合形成一个新的数据类型</li>
<li>可以通过匿名嵌入方式实现继承，从而共享代码和数据</li>
<li><p>匿名嵌入有三种方式</p>
<ul>
<li>接口类型</li>
<li>结构体实例</li>
<li>结构体实例指针</li>
</ul>
<p>接口类型更加灵活，只要实现这个接口的方法都可以进行赋值。</p>
<p>继承自其它结构体的struct类型可以直接访问父类结构体字段/方法</p>
</li>
<li><p>嵌入继承机制的局限</p>
<p>Golang从根本上阻止了抽象方法的使用。</p>
</li>
<li><p>多态性</p>
<p>golang不支持多态，即不能用子类替换父类。<br>但是golang支持接口类型的多态机制，只要结构体实现了接口的方法就可以进行赋值。</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://keything.github.io/2019/04/29/golang继承/" data-id="ckw1x7046002i5enupenh40uq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/golang/">golang</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-spark-Quick-Start" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/04/28/spark-Quick-Start/" class="article-date">
  <time datetime="2019-04-28T11:52:36.000Z" itemprop="datePublished">2019-04-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/04/28/spark-Quick-Start/">spark quick start</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>注意， Spark2.0 之前的版本，Spark的主要编程接口是RDD(Resilient Distributed Dataset)。2.0以后的版本，主要编程接口替换为Dataset。当然了RDD接口依然支持，可以从<a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" target="_blank" rel="noopener">RDD programming guide</a>。但是，我们强烈建议你切换到Dataset，比RDD有更好的性能。 关于Dataset可以从<a href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="noopener">SQL programming guide</a>获得更多细节。</p>
<h3 id="使用SparkShell进行交互分析"><a href="#使用SparkShell进行交互分析" class="headerlink" title="使用SparkShell进行交互分析"></a>使用SparkShell进行交互分析</h3><h4 id="基础内容"><a href="#基础内容" class="headerlink" title="基础内容"></a>基础内容</h4><p>启动 ./bin/spark-shell</p>
<p>基本执行：</p>
<pre><code>val textFile = spark.read.textFile(&quot;README.md&quot;)
textFile.count() // Number of items in this Dataset
textFile.first() // first item in this Dataset
</code></pre><p>将一个Dataset转换为新的一个。 调用<code>filter</code>返回新的Dataset，是原始文件的一个子集</p>
<pre><code>val linesWithSpark = textFile.filter(t =&gt; t.contains(&quot;Spark&quot;))
</code></pre><h4 id="这儿有个疑问，如何打印出dataset中内容？？"><a href="#这儿有个疑问，如何打印出dataset中内容？？" class="headerlink" title="这儿有个疑问，如何打印出dataset中内容？？"></a>这儿有个疑问，如何打印出dataset中内容？？</h4><h4 id="更多的Dataset的操作"><a href="#更多的Dataset的操作" class="headerlink" title="更多的Dataset的操作"></a>更多的Dataset的操作</h4><p>Dataset的actions和transformation 可以用于更多的复杂运算。</p>
<pre><code>textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a,b) =&gt; if (a &gt; b) a else b)
res4: Long = 15
</code></pre><p>map和reduce的参数都是scala的匿名函数，还可以使用scala/java 库。例如 可以使用<code>Math.max()</code>函数。</p>
<pre><code>import java.lang.Math
textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a, b) =&gt; Math.max(a,b))
</code></pre><p>一个通用的数据处理流程是MapReduce。Spark可以很容易的实现MapReduce流。 </p>
<pre><code>val wordCounts = textFile.map(line =&gt; line.split(&quot; &quot;)).groupByKey(identity).count()
</code></pre><h4 id="Caching"><a href="#Caching" class="headerlink" title="Caching"></a>Caching</h4><p>spark支持将dataset写入cluster-wide in-memory cache。<br>当数据重复获取时，这还是很有用的。 </p>
<h4 id="Self-Contained-Application"><a href="#Self-Contained-Application" class="headerlink" title="Self-Contained Application"></a>Self-Contained Application</h4><p>使用SparkAPI创建一个self-contained应用。</p>
<pre><code>import org.apache.spark.sql.SparkSession

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = &quot;README.md&quot; // Should be some file on your system
    val spark = SparkSession.builder.appName(&quot;Simple Application&quot;).getOrCreate()
    val logData = spark.read.textFile(logFile).cache()
    val numAs = logData.filter(line =&gt; line.contains(&quot;a&quot;)).count()
    val numBs = logData.filter(line =&gt; line.contains(&quot;b&quot;)).count()
    println(s&quot;Lines with a: $numAs, Lines with b: $numBs&quot;)
    spark.stop()
  }
}
</code></pre><p>其中<code>SparkSession.builder</code>构造一个[[SparkSession]]，使用设置application name，最后调用<code>getOrCreate</code>获取[[SparkSession]]实例。</p>
<p>也可以使用maven来进行包管理。 </p>
<p>目录结构：</p>
<pre><code>./pom.xml
./src
./src/main
./src/main/scala
./src/main/scala/didi
./src/main/scala/didi/map
./src/main/scala/didi/map/pointsys
./src/main/scala/didi/map/pointsys/App.scala
./src/test
./src/test/scala
./src/test/scala/didi
./src/test/scala/didi/map
./src/test/scala/didi/map/pointsys
</code></pre><p>App.scala内容：</p>
<pre><code>package didi.map.pointsys

import org.apache.spark.sql.SparkSession
object MyFunctions {
  def func1(s: String): String = {
    s.concat(&quot;yankai&quot;)
  }
}
// spark-submit --class=&quot;didi.map.pointsys.SimpleApp&quot; parking-user-profile-1.0-SNAPSHOT.jar
object SimpleApp {
  def main(args: Array[String]) {
    val logFile = &quot;README.md&quot; // Should be some file on your system
    val ss = SparkSession.builder().appName(&quot;Simple Application&quot;).enableHiveSupport().getOrCreate()
    val logData = ss.read.textFile(logFile)
    //val pairs = logData.map(s =&gt; (s, 1))

    val numAs = logData.filter(line =&gt; line.contains(&quot;a&quot;)).count()
    val numBs = logData.filter(line =&gt; line.contains(&quot;b&quot;)).count()
    println(s&quot;Lines with a: $numAs, Lines with b: $numBs&quot;)
    ss.stop()
  }
}
</code></pre><p>pom.xml</p>
<pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;groupId&gt;didi.map.pointsys&lt;/groupId&gt;
  &lt;artifactId&gt;parking-user-profile&lt;/artifactId&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
  &lt;inceptionYear&gt;2008&lt;/inceptionYear&gt;
  &lt;properties&gt;
    &lt;encoding&gt;UTF-8&lt;/encoding&gt;
    &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;
    &lt;scala.major.version&gt;2.11&lt;/scala.major.version&gt;
    &lt;deploy.scala.version&gt;2.11&lt;/deploy.scala.version&gt;
    &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;
    &lt;scala.compat.version&gt;2.11&lt;/scala.compat.version&gt;
    &lt;spark.version&gt;2.2.0&lt;/spark.version&gt;
  &lt;/properties&gt;

  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;junit&lt;/groupId&gt;
      &lt;artifactId&gt;junit&lt;/artifactId&gt;
      &lt;version&gt;4.12&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
      &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
      &lt;version&gt;2.3.0&lt;/version&gt;
      &lt;scope&gt;provided&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
      &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
      &lt;version&gt;2.3.0&lt;/version&gt;
      &lt;scope&gt;provided&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
      &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
      &lt;version&gt;2.7.2&lt;/version&gt;
      &lt;scope&gt;provided&lt;/scope&gt;
    &lt;/dependency&gt;

  &lt;/dependencies&gt;
  &lt;build&gt;
    &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;
    &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;
    &lt;plugins&gt;
      &lt;!-- bind the maven-assembly-plugin to the package phase this will create
          a jar file without the storm dependencies suitable for deployment to a cluster. --&gt;

      &lt;plugin&gt;
        &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;
        &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;
        &lt;version&gt;3.2.0&lt;/version&gt;
        &lt;executions&gt;
          &lt;execution&gt;
            &lt;goals&gt;
              &lt;goal&gt;compile&lt;/goal&gt;
              &lt;goal&gt;testCompile&lt;/goal&gt;
            &lt;/goals&gt;
          &lt;/execution&gt;
        &lt;/executions&gt;
        &lt;configuration&gt;
          &lt;scalaVersion&gt;${scala.version}&lt;/scalaVersion&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;

      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
        &lt;version&gt;2.2-beta-5&lt;/version&gt;
        &lt;configuration&gt;
          &lt;descriptorRefs&gt;
            &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
          &lt;/descriptorRefs&gt;
        &lt;/configuration&gt;
        &lt;executions&gt;
          &lt;execution&gt;
            &lt;phase&gt;package&lt;/phase&gt;
            &lt;goals&gt;
              &lt;goal&gt;single&lt;/goal&gt;
            &lt;/goals&gt;
          &lt;/execution&gt;
        &lt;/executions&gt;
      &lt;/plugin&gt;

      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
        &lt;version&gt;3.5.1&lt;/version&gt;
        &lt;configuration&gt;
          &lt;source&gt;1.8&lt;/source&gt;
          &lt;target&gt;1.8&lt;/target&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
      &lt;!-- disable surefire --&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
        &lt;version&gt;2.7&lt;/version&gt;
        &lt;configuration&gt;
          &lt;skipTests&gt;true&lt;/skipTests&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
      &lt;!-- enable scalatest --&gt;
      &lt;!-- &lt;plugin&gt;
        &lt;groupId&gt;org.scalatest&lt;/groupId&gt;
        &lt;artifactId&gt;scalatest-maven-plugin&lt;/artifactId&gt;
        &lt;version&gt;1.0&lt;/version&gt;
        &lt;configuration&gt;
          &lt;reportsDirectory&gt;${project.build.directory}/surefire-reports&lt;/reportsDirectory&gt;
          &lt;junitxml&gt;.&lt;/junitxml&gt;
          &lt;filereports&gt;WDF TestSuite.txt&lt;/filereports&gt;
        &lt;/configuration&gt;
        &lt;executions&gt;
          &lt;execution&gt;
            &lt;id&gt;test&lt;/id&gt;
            &lt;goals&gt;
              &lt;goal&gt;test&lt;/goal&gt;
            &lt;/goals&gt;
          &lt;/execution&gt;
        &lt;/executions&gt;
      &lt;/plugin&gt;
      --&gt;
    &lt;/plugins&gt;

    &lt;resources&gt;
      &lt;resource&gt;
        &lt;directory&gt;src/main/resources&lt;/directory&gt;
      &lt;/resource&gt;
    &lt;/resources&gt;

  &lt;/build&gt;
&lt;/project&gt;
</code></pre><p>在target目录下会生成jar文件，使用spark-submit进行提交。</p>
<pre><code>spark-submit --class=&quot;didi.map.pointsys.SimpleApp&quot; parking-user-profile-1.0-SNAPSHOT.jar
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://keything.github.io/2019/04/28/spark-Quick-Start/" data-id="ckw1x7052004q5enuggr4u302" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-golang-包导入" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/03/20/golang-包导入/" class="article-date">
  <time datetime="2019-03-20T11:52:36.000Z" itemprop="datePublished">2019-03-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/20/golang-包导入/">golang包导入</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="golang包导入"><a href="#golang包导入" class="headerlink" title="golang包导入"></a>golang包导入</h2><p>go源代码是按package方式组织，再通过import引入使用。</p>
<h3 id="工作目录"><a href="#工作目录" class="headerlink" title="工作目录"></a>工作目录</h3><p>在Go中代码保持在称之为workspace的系统文件夹中。这个工作目录有三个根目录：</p>
<ul>
<li>bin：包含可执行文件</li>
<li>pkg：包含不同操作系统架构的包二进制文件。</li>
<li>src：包含按包方式组织的源代码</li>
</ul>
<p>其中bin和pkg文件夹是在调用go命令安装和编译源代码时自动生成。</p>
<p>必须让Go知道工作目录的位置，这样才能知道包的具体位置。 通过设置环境变量GOPATH来指定。 </p>
<h3 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h3><ol>
<li><p>$GOPATH/src/importpackage/lib/lib.go</p>
<pre><code>package lib

import &quot;fmt&quot;

func SayHello() {
    fmt.Println(&quot;Hello,I&apos;m in myLib :) &quot;)
}
</code></pre></li>
<li><p>$GOPATH/src/importpackage/app/main.go</p>
<pre><code>package main

import &quot;importpackage/lib&quot;

func main() {
    lib.SayHello()
}
</code></pre></li>
<li><p>目录结构：</p>
<pre><code>.
└── src
    └── importpackage
        ├── app
        │   └── main.go
        └── lib
            └── lib.go
</code></pre><p> go build -o main src/importpackage/app/main.go             </p>
</li>
</ol>
<h3 id="导入包的多种方式"><a href="#导入包的多种方式" class="headerlink" title="导入包的多种方式"></a>导入包的多种方式</h3><ul>
<li>代码统一存储在工作目录下</li>
<li>工作目录里面有多个包，不同包按目录组织，包下面由多个代码文件组成。</li>
<li>导入包时按包的唯一路径进行导入，导入的包默认是必须要使用，如果不使用则编译失败，需要移除，减少不必要代码的引入，当然还有其他使用场景。默认情况下，我们使用文件名做为包名，方便理解。不同包组织不同的功能实现，方便理解。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://keything.github.io/2019/03/20/golang-包导入/" data-id="ckw1x7043002a5enu2wnwawt5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/golang/">golang</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-golang-与zookeeper交互" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/03/02/golang-与zookeeper交互/" class="article-date">
  <time datetime="2019-03-02T09:20:45.000Z" itemprop="datePublished">2019-03-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/02/golang-与zookeeper交互/">golang-与zookeeper交互</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>golang与zookeeper交互使用<a href="https://github.com/samuel/go-zookeeper/blob/master/examples/basic.go" target="_blank" rel="noopener">package github.com/samuel/go-zookeeper/zk</a></p>
<p>Connect函数定义</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://keything.github.io/2019/03/02/golang-与zookeeper交互/" data-id="ckw1x704200285enun5f2ms9x" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/golang/">golang</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-golang-面向对象编程" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/03/02/golang-面向对象编程/" class="article-date">
  <time datetime="2019-03-02T07:08:34.000Z" itemprop="datePublished">2019-03-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/02/golang-面向对象编程/">golang-面向对象编程</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>理解golang面向对象</p>
<p>面向对象编程的三个核心是：封装、继承、多态。</p>
<ol>
<li><p>封装(encapsulation)</p>
<p> 封装就是将抽象得到的数据和行为（或功能）相结合，形成一个有机的整体，也就是将数据与操作数据的源代码进行有机的结合，形成“类”，其中数据和函数都是类的成员。</p>
<p> 封装的目的是增强安全性和简化编程，使用者不必了解具体的实现细节，而只是要通过 外部接口，一特定的访问权限来使用类的成员。<br> 即不直接暴露数据，而暴露的是接口.</p>
<p> go封装是package层面的。小写开头的Names只能在包内可见。在一个private package可以隐藏所有东西，并只暴露特定类型、接口、工厂函数。</p>
</li>
</ol>
<ol start="2">
<li><p>继承</p>
<p> 继承是指一个对象直接使用另一对象的属性和方法。事实上，我们遇到的很多实体都有继承的含义。例如，若把汽车看成一个实体，它可以分成多个子实体，如：卡车、公共汽车等。这些子实体都具有汽车的特性，因此，汽车是它们的“父亲”，而这些子实体则是汽车的“孩子”。</p>
</li>
</ol>
<pre><code>现代语言认为实现继承更好的方式是组合(composition)。go采用这种理念，并且没有任何等级内容(hierarchy)。 这允许你使用组合来共享实现的细节。go是通过嵌入(embedding)的方式来实现匿名组合的(anonymous composition)。 

通过嵌入一个匿名类型的组合实现了继承。 嵌入的结构体等同于基类(base class)。当然了也可以嵌入一个接口，但是必须注意，嵌入一个接口时，该结构体必须实现这个接口的方法，不然会报runtime error。

报错：panic: runtime error: invalid memory address or nil pointer dereference
</code></pre><ol start="3">
<li><p>多态（polymorphism）</p>
<p> 多态是允许你将父对象设置成为和一个或更多的子对象相等的技术。赋值会后，父对象就可以根据当前赋值给它的子对象以不同的方式运作。简单来说，允许将子类类型的指针赋值给父类类型的指针。 在C++中都是通过虚函数(Virtual Function)实现的。golang允许接口的子类的多态，但不允许子类替换为父类</p>
</li>
</ol>
<h3 id="实际例子"><a href="#实际例子" class="headerlink" title="实际例子"></a>实际例子</h3><p>在我们实际项目中用到多态的地方很多，举一个例子 获取下游服务的节点列表：<br>需求：</p>
<ul>
<li>希望支持多种方式获取节点列表，比如配置文件、服务发现、http请求等；</li>
<li>希望通过配置获取顺序的方式来实现优先级，比如配置是<code>get_type=服务发现,配置文件,http请求</code>。那么当服务发现获取节点成功时，则使用服务发现；反之如果服务发现获取节点列表失败，则需要使用配置文件的方式。</li>
</ul>
<p>抽象：</p>
<ul>
<li>定义一个接口IConfObj：</li>
<li><p>check函数：为了检查配置文件的配置项是否完备，因为不通获取方式，配置文件不一样；</p>
<pre><code>+ run函数：执行节点获取，并执行 InterfaceAction来执行节点更新。
</code></pre><ul>
<li><p>定义一个获取节点后动作的func，目的就是在获取节点后，通过该func进行操作。         </p>
<pre><code>type IConfObj interface {
    check() error
    run() (bool, error)
}

type InterfaceAction func(hosts []string, is_high_node, is_primary bool) (bool, error)
</code></pre></li>
<li><p>实现节点列表获取配置化</p>
<ul>
<li>不同的获取获取方式实现这个接口，并进行注册。</li>
<li>根据注册先后顺序执行run，如果run成功则结束遍历，反之继续执行直至成功。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>参考文章：</p>
<p><a href="https://code.tutsplus.com/tutorials/lets-go-object-oriented-programming-in-golang--cms-26540" target="_blank" rel="noopener">https://code.tutsplus.com/tutorials/lets-go-object-oriented-programming-in-golang--cms-26540</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://keything.github.io/2019/03/02/golang-面向对象编程/" data-id="ckw1x7043002b5enucuxz3fp3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/golang/">golang</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hive的TRANSFORM操作" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/23/hive的TRANSFORM操作/" class="article-date">
  <time datetime="2018-09-23T15:03:39.000Z" itemprop="datePublished">2018-09-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/23/hive的TRANSFORM操作/">hive的TRANSFORM操作</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Transform-Map-Reduce-Syntax"><a href="#Transform-Map-Reduce-Syntax" class="headerlink" title="Transform/Map-Reduce Syntax"></a>Transform/Map-Reduce Syntax</h2><p>hive语言中内置的特性是支持用户自定义mappers/redulers的。 用户可以使用<code>TRANSFROM</code> 子句来内嵌mapper/reduer脚本的。 </p>
<p>By default, columns will be transformed to STRING and delimited by TAB before feeding to the user script; similarly, all NULL values will be converted to the literal string \N in order to differentiate NULL values from empty strings. The standard output of the user script will be treated as TAB-separated STRING columns, any cell containing only \N will be re-interpreted as a NULL, and then the resulting STRING column will be cast to the data type specified in the table declaration in the usual way. User scripts can output debug information to standard error which will be shown on the task detail page on hadoop. These defaults can be overridden with ROW FORMAT ….</p>
<p>注意：</p>
<p>Formally, MAP … and REDUCE … are syntactic transformations of SELECT TRANSFORM ( … ). In other words, they serve as comments or notes to the reader of the query. BEWARE: Use of these keywords may be dangerous as (e.g.) typing “REDUCE” does not force a reduce phase to occur and typing “MAP” does not force a new map phase!</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">clusterBy: CLUSTER BY colName (&apos;,&apos; colName)*</span><br><span class="line">distributeBy: DISTRIBUTE BY colName (&apos;,&apos; colName)*</span><br><span class="line">sortBy: SORT BY colName (ASC | DESC)? (&apos;,&apos; colName (ASC | DESC)?)*</span><br><span class="line"> </span><br><span class="line">rowFormat</span><br><span class="line">  : ROW FORMAT</span><br><span class="line">    (DELIMITED [FIELDS TERMINATED BY char]</span><br><span class="line">               [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">               [MAP KEYS TERMINATED BY char]</span><br><span class="line">               [ESCAPED BY char]</span><br><span class="line">               [LINES SEPARATED BY char]</span><br><span class="line">     |</span><br><span class="line">     SERDE serde_name [WITH SERDEPROPERTIES</span><br><span class="line">                            property_name=property_value,</span><br><span class="line">                            property_name=property_value, ...])</span><br><span class="line"> </span><br><span class="line">outRowFormat : rowFormat</span><br><span class="line">inRowFormat : rowFormat</span><br><span class="line">outRecordReader : RECORDREADER className</span><br><span class="line"> </span><br><span class="line">query:</span><br><span class="line">  FROM (</span><br><span class="line">    FROM src</span><br><span class="line">    MAP expression (&apos;,&apos; expression)*</span><br><span class="line">    (inRowFormat)?</span><br><span class="line">    USING &apos;my_map_script&apos;</span><br><span class="line">    ( AS colName (&apos;,&apos; colName)* )?</span><br><span class="line">    (outRowFormat)? (outRecordReader)?</span><br><span class="line">    ( clusterBy? | distributeBy? sortBy? ) src_alias</span><br><span class="line">  )</span><br><span class="line">  REDUCE expression (&apos;,&apos; expression)*</span><br><span class="line">    (inRowFormat)?</span><br><span class="line">    USING &apos;my_reduce_script&apos;</span><br><span class="line">    ( AS colName (&apos;,&apos; colName)* )?</span><br><span class="line">    (outRowFormat)? (outRecordReader)?</span><br><span class="line"> </span><br><span class="line">  FROM (</span><br><span class="line">    FROM src</span><br><span class="line">    SELECT TRANSFORM &apos;(&apos; expression (&apos;,&apos; expression)* &apos;)&apos;</span><br><span class="line">    (inRowFormat)?</span><br><span class="line">    USING &apos;my_map_script&apos;</span><br><span class="line">    ( AS colName (&apos;,&apos; colName)* )?</span><br><span class="line">    (outRowFormat)? (outRecordReader)?</span><br><span class="line">    ( clusterBy? | distributeBy? sortBy? ) src_alias</span><br><span class="line">  )</span><br><span class="line">  SELECT TRANSFORM &apos;(&apos; expression (&apos;,&apos; expression)* &apos;)&apos;</span><br><span class="line">    (inRowFormat)?</span><br><span class="line">    USING &apos;my_reduce_script&apos;</span><br><span class="line">    ( AS colName (&apos;,&apos; colName)* )?</span><br><span class="line">    (outRowFormat)? (outRecordReader)?</span><br></pre></td></tr></table></figure>
<p>转化的例子1：</p>
<p><code>FROM (
  FROM pv_users
  MAP pv_users.userid, pv_users.date
  USING &#39;map_script&#39;
  AS dt, uid
  CLUSTER BY dt) map_output
INSERT OVERWRITE TABLE pv_users_reduced
  REDUCE map_output.dt, map_output.uid
  USING &#39;reduce_script&#39;
  AS date, count;
FROM (
  FROM pv_users
  SELECT TRANSFORM(pv_users.userid, pv_users.date)
  USING &#39;map_script&#39;
  AS dt, uid
  CLUSTER BY dt) map_output
INSERT OVERWRITE TABLE pv_users_reduced
  SELECT TRANSFORM(map_output.dt, map_output.uid)
  USING &#39;reduce_script&#39;
  AS date, count;</code></p>
<p>转化的例子2：</p>
<p><code>FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value) ROW FORMAT SERDE &#39;org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe&#39;
  USING &#39;/bin/cat&#39;
  AS (tkey, tvalue) ROW FORMAT SERDE &#39;org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe&#39;
  RECORDREADER &#39;org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader&#39;
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue</code></p>
<p>将TRANSFORM的输出打出来</p>
<p>脚本的输出默认是string，如果想进行类型转换的需要进行如下操作。</p>
<p><code>SELECT TRANSFORM(stuff)
USING &#39;script&#39;
AS thing1, thing2</code></p>
<p>类型转化<br><code>SELECT TRANSFORM(stuff)
USING &#39;script&#39;
AS (thing1 INT, thing2 INT)</code></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://keything.github.io/2018/09/23/hive的TRANSFORM操作/" data-id="ckw1x7049002q5enu6p7m4szo" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-hive权威指南学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/23/hive权威指南学习笔记/" class="article-date">
  <time datetime="2018-09-23T15:02:05.000Z" itemprop="datePublished">2018-09-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/23/hive权威指南学习笔记/">hive权威指南学习笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Hive操作</p>
<p>本篇文章是对 <code>https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-DDLOperations</code> </p>
<p>以及</p>
<p>《Hive编程指南》的学习总结</p>
<ol>
<li>DDL Operations (data defination language)</li>
</ol>
<ul>
<li>create table pokes (foo INT, bar STRING);</li>
<li>create table invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);</li>
</ul>
<p>partition column 是一个虚拟列，并不是数据本身的一部分，但是可以获得某一份特定数据集。 </p>
<ul>
<li>show tables;</li>
<li>show tables ‘.*s’</li>
<li>describe invites;</li>
</ul>
<ol start="2">
<li>DML Operations (data manipulation language)</li>
</ol>
<h3 id="2-1-装载数据"><a href="#2-1-装载数据" class="headerlink" title="2.1 装载数据"></a>2.1 装载数据</h3><ul>
<li>LOAD DATA LOCAL INPUT ‘./examples/files/kv1.txt’ OVERWRITE INTO TABLE pokes;</li>
<li>LOAD DATA LOCAL INPATH ‘./examples/files/kv2.txt’ OVERWRITE INTO TABLE invites PARTITION (ds=’2008-08-15’);</li>
<li>LOAD DATA LOCAL INPATH ‘./examples/files/kv3.txt’ OVERWRITE INTO TABLE invites PARTITION (ds=’2008-08-08’);</li>
</ul>
<p><code>LOCAL</code> 表明这是在本地文件系统上的文件；如果没有的话，那么就从hdfs上获取文件。</p>
<p><code>OVERWRITE</code> 表明表中已经存在的数据将会被删除。 如果没有的话，数据文件被添加到已经存在的数据集合中。 </p>
<h3 id="2-2-通过查询语句插入数据"><a href="#2-2-通过查询语句插入数据" class="headerlink" title="2.2 通过查询语句插入数据"></a>2.2 通过查询语句插入数据</h3><ul>
<li><code>FROM staged_employees se 
INSERT OVERWRITE TABLE employee PARTITION (country = &#39;US&#39; and state = &#39;OR&#39;) select *  where se.country = &#39;US&#39; and se.state = &#39;OR&#39;
INSERT OVERWRITE TABLE employee PARTITION (country = &#39;US&#39; and state = &#39;CA&#39;) select *  where se.country = &#39;US&#39; and se.state = &#39;CA&#39;</code></li>
</ul>
<h4 id="动态分区插入"><a href="#动态分区插入" class="headerlink" title="动态分区插入"></a>动态分区插入</h4><p>前面的语法中有一个问题，即：如果需要创建非常多的分区，那么用户就需要写非常多的SQL。<br>幸运的是：Hive提供了一个动态分区功能，可以基于查询参数推断出需要创建的分区名称。相比之下，前面看到的都是静态分区。 </p>
<p><code>INSERT OVERWRITE TABLE employees PARTITION (country, state) 
 SELECT ..., se.cnty, se.st FROM staged_employees se;</code></p>
<p>注意：Hive 根据SELECT语句中最后2列来确定分区字段country和state的值。<br>而不是根据命名来匹配的。</p>
<p>在SQL语句使用不同的命名就是为了强调源表字段值和输出分区值直接的关系是根据位置而不是根据命名来匹配的。 </p>
<h3 id="2-3-单个查询语句中创建表并加载数据"><a href="#2-3-单个查询语句中创建表并加载数据" class="headerlink" title="2.3 单个查询语句中创建表并加载数据"></a>2.3 单个查询语句中创建表并加载数据</h3><ul>
<li>create table u_data_v2 as SELECT userid, rating from u_data limit 10;</li>
</ul>
<h3 id="2-4-导出数据"><a href="#2-4-导出数据" class="headerlink" title="2.4 导出数据"></a>2.4 导出数据</h3><ul>
<li>INSERT OVERWRITE LOCAL DIRECTORY ‘/tmp/hive’ SELECT userid, rating FROM u_data; </li>
</ul>
<ol start="3">
<li>SQL操作</li>
</ol>
<h3 id="SELECT-…-FROM-语句"><a href="#SELECT-…-FROM-语句" class="headerlink" title="SELECT … FROM 语句"></a>SELECT … FROM 语句</h3><p>3.1 数据类型</p>
<p> 当用户选择的列是集合数据类型时，Hive会使用JSON语法进行输出。</p>
<ul>
<li>当列是一个数组时，其值使用一个括在<code>[...]</code>的逗号分隔的列进行表示，如[“Mary Smith”, “Todd Jones”]</li>
<li>当列是一个Map时，使用JSON格式来表达map，即使用一个被括在<code>{...}</code>内的以逗号分隔的键值对列表进行表示；如 <code>{&quot;Federal Taxes&quot;:0.2 &quot;State Taxes&quot;: 0.05}</code></li>
<li>当列是一个Struct时，使用map格式来表示，如<code>{&quot;Federal Taxes&quot;:0.2 &quot;State Taxes&quot;: 0.05}</code></li>
</ul>
<p>引用元素的方式：</p>
<ul>
<li>引用Map元素，使用ARRAY[…]语法；</li>
<li>引用Struct的一个元素，使用<code>点</code>符号。</li>
</ul>
<p>3.2 使用函数</p>
<p>3.3 列别名</p>
<p>如果新产生的结果在源表中不存在的话，通常有必要给这些新产生的列起一个名称，也就是别名。 </p>
<ul>
<li>select userid as uid from u_data;</li>
</ul>
<p>3.4 嵌套SELECT 语句</p>
<p><code>FROM (
    SELECT upper(name), salary ,deductions[&quot;Federal Taxes&quot;] as fed_taxes FROM employees
    ) e
SELECT e.name, e.fed_taxes where e.name = &quot;yan&quot;</code></p>
<p>从这个嵌套语句中可以看到，我们将前面的结果集起了个别名，称之为e，在这个语句外面嵌套查询了name fed_taxes两个字段。 同时约束name必须是yan。</p>
<p>3.5 CASE … WHEN … THEN 句式</p>
<p>CASE .. WHEN .. THEN 语句和if条件语句类似，用于处理单个列的查询结果。 </p>
<p><code>SELECT userid, movieid, 
    CASE 
    WHEN rating &lt;= 1 THEN &quot;low&quot; 
    WHEN rating &gt; 1 and rating &lt;= 3 THEN &quot;middle&quot; ELSE &quot;high&quot;  
    END  as level 
    from u_data limit 10;</code></p>
<h3 id="4-WHERE-语句-（对应Hive编程指南的6-2-WHERE语句）"><a href="#4-WHERE-语句-（对应Hive编程指南的6-2-WHERE语句）" class="headerlink" title="4. WHERE 语句 （对应Hive编程指南的6.2 WHERE语句）"></a>4. WHERE 语句 （对应Hive编程指南的6.2 WHERE语句）</h3><h4 id="4-1-WHERE语句不能使用列别名。"><a href="#4-1-WHERE语句不能使用列别名。" class="headerlink" title="4.1  WHERE语句不能使用列别名。"></a>4.1  WHERE语句不能使用<code>列别名</code>。</h4><p><code>SELECT name, salary,  salary * (1-deductions[&quot;Federal Taxes&quot;]) as salary_minus_fed_taxes 
FROM employees
WHERE round(salary * (1-deductions[&quot;Federal Taxes&quot;])) &gt; 70000</code></p>
<p>这个查询语句里面，有重复的表达式。下面的查询语句使用一个列别名来消除重复问题，但是并不能生效。 </p>
<p><code>SELECT name, salary,  salary * (1-deductions[&quot;Federal Taxes&quot;]) as salary_minus_fed_taxes 
FROM employees
WHERE round(salary_minus_fed_taxes) &gt; 70000;
报错：Invalid table alias or column reference &#39;salary_minus_fed_taxes&#39;</code></p>
<p>正如错误信息所提示的，WHERE语句不能使用<code>列别名</code>。<br>不过我们可以使用一个嵌套的SELECT语句</p>
<p><code>SELECT e.* FROM 
(SELECT name,salary, salary * (1-deductions[&quot;Federal Taxes&quot;]) as salary_minus_fed_taxes FROM employees) e 
WHERE round(e.salary_minus_fed_taxes) &gt; 70000;</code></p>
<h4 id="4-2-谓词操作符"><a href="#4-2-谓词操作符" class="headerlink" title="4.2 谓词操作符"></a>4.2 谓词操作符</h4><h4 id="4-3-GROUP-BY语句"><a href="#4-3-GROUP-BY语句" class="headerlink" title="4.3 GROUP BY语句"></a>4.3 GROUP BY语句</h4><p>GROUP BY 通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。 </p>
<h3 id="4-4-JOIN-语句"><a href="#4-4-JOIN-语句" class="headerlink" title="4.4 JOIN 语句"></a>4.4 JOIN 语句</h3><p>Hive支持通常的SQL JOIN语句，但是只支持<code>等值连接</code>，并且在ON子句中只支持AND。</p>
<p>4.4.1 INNER JOIN</p>
<p><code>SELECT a.ymd, a.price_close, b.price_close, c.price_close 
FROM stocks a JOIN stocks b ON a.ymd = b.ymd
              JOIN stocks c ON a.ymd = c.ymd
WHERE a.symbol = &#39;AAPL&#39; AND b.symbol = &#39;IBM&#39; AND c.symbol = &#39;GE&#39;;</code></p>
<p>在这个例子中，会首先启动一个MapReduce job对表a和表b进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表c进行连接操作。 </p>
<p>Hive表总是按照从左到右的顺序执行的。 </p>
<h4 id="JOIN优化"><a href="#JOIN优化" class="headerlink" title="JOIN优化"></a>JOIN优化</h4><p>Hive同时假定查询中最后一个表是最大的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。</p>
<p>因此用户需要保证<code>连续查询中的表的大小是从左到右依次增加的</code>。</p>
<h4 id="LEFT-OUTER-JOIN"><a href="#LEFT-OUTER-JOIN" class="headerlink" title="LEFT OUTER JOIN"></a>LEFT OUTER JOIN</h4><h4 id="OUTER-JOIN"><a href="#OUTER-JOIN" class="headerlink" title="OUTER JOIN"></a>OUTER JOIN</h4><h4 id="RIGHT-OUTER-JOIN"><a href="#RIGHT-OUTER-JOIN" class="headerlink" title="RIGHT OUTER JOIN"></a>RIGHT OUTER JOIN</h4><h4 id="FULL-OUTER-JOIN"><a href="#FULL-OUTER-JOIN" class="headerlink" title="FULL OUTER JOIN"></a>FULL OUTER JOIN</h4><h4 id="LEFT-SEMI-JOIN"><a href="#LEFT-SEMI-JOIN" class="headerlink" title="LEFT SEMI-JOIN"></a>LEFT SEMI-JOIN</h4><h4 id="笛卡尔积JOIN"><a href="#笛卡尔积JOIN" class="headerlink" title="笛卡尔积JOIN"></a>笛卡尔积JOIN</h4><h4 id="map-side-JOIN"><a href="#map-side-JOIN" class="headerlink" title="map-side JOIN"></a>map-side JOIN</h4><h3 id="4-5-ORDER-BY-和-SORT-BY"><a href="#4-5-ORDER-BY-和-SORT-BY" class="headerlink" title="4.5 ORDER BY 和 SORT BY"></a>4.5 ORDER BY 和 SORT BY</h3><h3 id="4-6-含有SORT-BY-的DISTRIBUTE-BY"><a href="#4-6-含有SORT-BY-的DISTRIBUTE-BY" class="headerlink" title="4.6 含有SORT BY 的DISTRIBUTE BY"></a>4.6 含有SORT BY 的DISTRIBUTE BY</h3><h3 id="4-10-UNION-ALL"><a href="#4-10-UNION-ALL" class="headerlink" title="4.10 UNION ALL"></a>4.10 UNION ALL</h3><p>UNION ALL将2个或多个进行合并。每个union子查询都必须有相同的列，并且每个字段的字段类型必须是一致的。</p>
<p>比如，如果第2个字段是FLOAT类型，那么所有其他子查询的第2个字段必须都是FLOAT类型的。</p>
<p>举例，将日志数据进行合并的例子:</p>
<p><code>SELECT log.ymd, log.level, log.message 
    FROM(
            SELECT l1.ymd, l1.level, l1.message, &#39;Log1&#39; AS source
            FROM log1 l1
        UNION ALL
            SELECT l2.ymd, l2.level, l2.message, &#39;Log2&#39; AS source
            FROM log2 l2 
        ) log
    SORT BY log.ymd ASC;</code></p>
<h2 id="5-视图（hive编程指南第7章-视图）"><a href="#5-视图（hive编程指南第7章-视图）" class="headerlink" title="5 视图（hive编程指南第7章 视图）"></a>5 视图（hive编程指南第7章 视图）</h2><p>3.3 一些用法</p>
<ul>
<li>SELECT a.* FROM invites a WHERE a.ds=’2008-08-15’;</li>
<li>FROM pokes t1 JOIN invites t2 ON (t1.bar = t2.bar) INSERT OVERWRITE TABLE events SELECT t1.bar, t1.foo, t2.foo;</li>
<li><p>CREATE TABLE u_data (<br>userid INT,<br>movieid INT,<br>rating INT,<br>unixtime STRING)<br>ROW FORMAT DELIMITED<br>FIELDS TERMINATED BY ‘\t’<br>STORED AS TEXTFILE;</p>
</li>
<li><p>LOAD DATA LOCAL INPATH ‘/Users/keything/Downloads/ml-100k/u.data’ OVERWRITE INTO TABLE u_data;</p>
</li>
<li><code>SELECT COUNT(*) FROM u_data;</code></li>
</ul>
<p>一个简单的例子：</p>
<p>创建表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE u_data (</span><br><span class="line">  userid INT,</span><br><span class="line">  movieid INT,</span><br><span class="line">  rating INT,</span><br><span class="line">  unixtime STRING)</span><br><span class="line">ROW FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">STORED AS TEXTFILE;</span><br></pre></td></tr></table></figure>
<p>获取数据集</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://files.grouplens.org/datasets/movielens/ml-100k.zip</span><br><span class="line">unzip ml-100k.zip</span><br></pre></td></tr></table></figure>
<p>加载数据集</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &apos;&lt;path&gt;/u.data&apos; OVERWRITE INTO TABLE u_data;</span><br></pre></td></tr></table></figure>
<p>创建weekday_mapper.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">for line in sys.stdin:</span><br><span class="line">  line = line.strip()</span><br><span class="line">  userid, movieid, rating, unixtime = line.split(&apos;\t&apos;)</span><br><span class="line">  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()</span><br><span class="line">  print &apos;\t&apos;.join([userid, movieid, rating, str(weekday)])</span><br></pre></td></tr></table></figure>
<p>对于这个脚本是否正确，可以通过自己创建文件来验证。</p>
<p>创建mapper脚本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE u_data_new (</span><br><span class="line">  userid INT,</span><br><span class="line">  movieid INT,</span><br><span class="line">  rating INT,</span><br><span class="line">  weekday INT)</span><br><span class="line">ROW FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED BY &apos;\t&apos;;</span><br><span class="line"></span><br><span class="line">add FILE weekday_mapper.py;</span><br><span class="line"></span><br><span class="line">INSERT OVERWRITE TABLE u_data_new</span><br><span class="line">SELECT</span><br><span class="line">  TRANSFORM (userid, movieid, rating, unixtime)</span><br><span class="line">  USING &apos;python weekday_mapper.py&apos;</span><br><span class="line">  AS (userid, movieid, rating, weekday)</span><br><span class="line">FROM u_data;</span><br><span class="line"></span><br><span class="line">SELECT weekday, COUNT(*)</span><br><span class="line">FROM u_data_new</span><br><span class="line">GROUP BY weekday;</span><br></pre></td></tr></table></figure>
<p>最后得到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1	12254</span><br><span class="line">2	13579</span><br><span class="line">3	14430</span><br><span class="line">4	15114</span><br><span class="line">5	14743</span><br><span class="line">6	18229</span><br><span class="line">7	11651</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://keything.github.io/2018/09/23/hive权威指南学习笔记/" data-id="ckw1x7048002o5enu0svcw2be" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-hadoop权威指南第四版学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/23/hadoop权威指南第四版学习笔记/" class="article-date">
  <time datetime="2018-09-23T03:30:21.000Z" itemprop="datePublished">2018-09-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/23/hadoop权威指南第四版学习笔记/">hadoop权威指南第四版学习笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Hadoop权威指南第四版-学习笔记"><a href="#Hadoop权威指南第四版-学习笔记" class="headerlink" title="Hadoop权威指南第四版-学习笔记"></a>Hadoop权威指南第四版-学习笔记</h1><h2 id="第二章-关于MapReduce"><a href="#第二章-关于MapReduce" class="headerlink" title="第二章 关于MapReduce"></a>第二章 关于MapReduce</h2><p>MapReduce是一种可用于数据处理的编程模型。 </p>
<p>Hadoop将MapReduce的输入数据划分成等长的小数据块，称为输入分片（input split) 或简称“分片”。hadoop为每个分片构建一个map任务，并由该任务来运行用户自定义的map函数，从而处理分片中的每条记录。 </p>
<p>一个合理的分片大小趋向于HDFS的一个块的大小，默认是128MB。</p>
<h4 id="map任务将其输出写入本地硬盘，而非HDFS。这是为什么呢-？"><a href="#map任务将其输出写入本地硬盘，而非HDFS。这是为什么呢-？" class="headerlink" title="map任务将其输出写入本地硬盘，而非HDFS。这是为什么呢 ？"></a>map任务将其输出写入本地硬盘，而非HDFS。这是为什么呢 ？</h4><p>因为map的输出是中间结果：该中间结果由reduce任务处理后才产生最终输出结果，而且一旦作业完成，map的输出结果就可以删除。 因此，如果把map输出存储在HDFS中并实现备份，难免有些小题大做。 </p>
<p>如果运行map任务的节点在map中间结果传送给reduce任务之前失败，hadoop将在另一个节点重新运行这个map任务以再次构建map中间结果。</p>
<p>reduce任务并不具备数据本地化的优势，单个reduce任务的输入通常来自于所有mapper的输出。</p>
<p>reduce任务的数量并不是由输入数据的大小决定，相反是独立指定的。<br>如果有多个reduce任务，每个map任务就会针对输出进行分区(partition)，即为每个reduce任务建一个分区，每个分区有许多键（及其对应的值），但每个键对应的key-value 都在同一个分区中。 用户可以使用自定义的分区函数控制分区，但通常使用默认的partitioner 通过哈希函数来区分，很高效。</p>
<p>一般情况下，多个reduce任务的数据流如图2-4所示，该图清楚表明了为什么map任务和reduce任务之间的数据流成为shuffle（混洗），因为每个reduce任务的输入都来自许多map任务。shuffle一般比图中所示的更复杂，而且调整混洗参数对作业总执行时间的影响非常大。 </p>
<h2 id="第三章-Hadoop分布式文件系统"><a href="#第三章-Hadoop分布式文件系统" class="headerlink" title="第三章 Hadoop分布式文件系统"></a>第三章 Hadoop分布式文件系统</h2><h3 id="3-2-HDFS概念"><a href="#3-2-HDFS概念" class="headerlink" title="3.2 HDFS概念"></a>3.2 HDFS概念</h3><p>3.2.1 数据块</p>
<p>HDFS同样也有块block的概念，默认是128MB。</p>
<p>3.2.2 namenode和datanode</p>
<p>HDFS集群有两类节点，以 管理节点-工作节点 模式运行，即一个namenode（管理节点）和多个datanode（工作节点）。<br>namenode 管理文件系统的命名空间。它维护者文件系统树及整颗树内所有的文件和目录。这些信息以两个文件形式永久保存在本地磁盘上：命名空间镜像文件和编辑日志文件。 </p>
<p>namenode也记录着每个文件中各个块所在的数据节点信息，但它并不用就保存块的位置信息，因为这些信息会在系统启动时根据数据节点信息重建。 </p>
<p>datanode是文件系统的工作节点。他们根据需要存储并检索数据块（受客户端或namenode调度），并且定期向namenode发送所存储的块的列表。</p>
<p>3.2.3 块缓存<br>3.2.4 联邦HDFS<br>3.2.5 HDFS的高可用性</p>
<h3 id="3-3-命令行接口"><a href="#3-3-命令行接口" class="headerlink" title="3.3 命令行接口"></a>3.3 命令行接口</h3><h3 id="3-4-Hadoop-文件系统"><a href="#3-4-Hadoop-文件系统" class="headerlink" title="3.4 Hadoop 文件系统"></a>3.4 Hadoop 文件系统</h3><h3 id="3-5-Java接口"><a href="#3-5-Java接口" class="headerlink" title="3.5 Java接口"></a>3.5 Java接口</h3><h3 id="3-6-数据流"><a href="#3-6-数据流" class="headerlink" title="3.6 数据流"></a>3.6 数据流</h3><p>3.6.1 剖析文件读取<br>3.6.2 剖析文件写入<br>3.6.3 一致性模型</p>
<h2 id="第4章-关于YARN"><a href="#第4章-关于YARN" class="headerlink" title="第4章  关于YARN"></a>第4章  关于YARN</h2><p>Apache YARN(Yet Another Resource Negotiator的缩写)是Hadoop的集群资源管理系统。</p>
<h2 id="第5章-Hadoop的I-O-操作"><a href="#第5章-Hadoop的I-O-操作" class="headerlink" title="第5章 Hadoop的I/O 操作"></a>第5章 Hadoop的I/O 操作</h2><p>5.1 数据完整性<br>5.2 压缩</p>
<p>5.2.1 codec<br>5.2.2 压缩和输入分片</p>
<p>5.4 基于文件的数据结构</p>
<h2 id="第6章-MapReduce应用开发"><a href="#第6章-MapReduce应用开发" class="headerlink" title="第6章 MapReduce应用开发"></a>第6章 MapReduce应用开发</h2><p>MapReduce编程遵循一个特定的流程。首先写map函数和reduce函数，最好使用单元测试来确保函数的运行符合预期。 然后写一个驱动程序来运行作业。</p>
<h2 id="第7章-MapReduce的工作机制"><a href="#第7章-MapReduce的工作机制" class="headerlink" title="第7章 MapReduce的工作机制"></a>第7章 MapReduce的工作机制</h2><h3 id="7-3-shuffle和排序"><a href="#7-3-shuffle和排序" class="headerlink" title="7.3 shuffle和排序"></a>7.3 shuffle和排序</h3><p>MapReduce确保每个reducer的输入都是按键排序的。</p>
<p>系统执行排序、将map输出作为输入传给reducer的过程称为shuffle。</p>
<p>7.3.1 map端</p>
<p>map函数开始产生输出时，并不是简单地将它写到磁盘。这个过程更复杂，利用缓冲的方式写到内存并出于效率的考虑进行预排序。<br>每个map任务都有一个环形内存缓冲区用于存储任务输出。在默认情况下，缓冲区大小是100mb。一旦缓冲内容达到阈值，一个后台线程便开始把内容溢出到splill到磁盘。</p>
<p>在溢出写到磁盘过程中，map输出继续写到缓存区，但如果在此期间缓冲区被填满，map会被阻塞直到写磁盘过程完成。 </p>
<p>在写磁盘之前，线程首先根据数据最终要传的reduer 将数据划分成相应的分区(partition)。在每个分区中，后台线程按键进行内存中排序，如果有一个combiner函数，它就在排序后的输出上运行。运行combiner函数使得map输出结果更紧凑。</p>
<p>每次内存缓冲区达到溢出阈值，就会新建一个溢出文件spill file，因此在map任务写完最后一个输出记录之后，会有几个溢出文件。</p>

<p>7.3.2 reduce端</p>
<h3 id="7-4-任务的执行"><a href="#7-4-任务的执行" class="headerlink" title="7.4 任务的执行"></a>7.4 任务的执行</h3><h2 id="第8章"><a href="#第8章" class="headerlink" title="第8章"></a>第8章</h2><p>8.1 MapReduce的类型</p>
<p>Hadoop的MapReduce中，map函数和reduce函数遵循如下常规格式：</p>
<p>map:(k1, v1) -&gt; list(k2, v2)<br>combiner: (k2, list(v2)) -&gt; list(k2, v2)<br>reduce:(k2, list(v2)) -&gt; list(k3, v3)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://keything.github.io/2018/09/23/hadoop权威指南第四版学习笔记/" data-id="ckw1x7048002m5enuzmjss2wc" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Laravel/">Laravel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/">Redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/composer/">composer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cpp/">cpp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cpp-lib/">cpp-lib</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/elasticsearch/">elasticsearch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/faiss/">faiss</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/">git</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/go-source-code/">go.source.code</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/golang/">golang</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/laravel/">laravel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mac/">mac</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/">machine-learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/memcache/">memcache</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nginx/">nginx</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nosql/">nosql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/octave/">octave</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/php/">php</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/php源码学习/">php源码学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/">redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tcp/">tcp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/thrift/">thrift</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/分布式/">分布式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/基础知识/">基础知识</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Laravel/" style="font-size: 10px;">Laravel</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/composer/" style="font-size: 10px;">composer</a> <a href="/tags/cpp/" style="font-size: 12.22px;">cpp</a> <a href="/tags/cpp-lib/" style="font-size: 10px;">cpp-lib</a> <a href="/tags/elasticsearch/" style="font-size: 11.11px;">elasticsearch</a> <a href="/tags/faiss/" style="font-size: 10px;">faiss</a> <a href="/tags/git/" style="font-size: 12.22px;">git</a> <a href="/tags/go-source-code/" style="font-size: 10px;">go.source.code</a> <a href="/tags/golang/" style="font-size: 16.67px;">golang</a> <a href="/tags/laravel/" style="font-size: 11.11px;">laravel</a> <a href="/tags/linux/" style="font-size: 14.44px;">linux</a> <a href="/tags/mac/" style="font-size: 13.33px;">mac</a> <a href="/tags/machine-learning/" style="font-size: 10px;">machine-learning</a> <a href="/tags/memcache/" style="font-size: 15.56px;">memcache</a> <a href="/tags/mysql/" style="font-size: 13.33px;">mysql</a> <a href="/tags/nginx/" style="font-size: 13.33px;">nginx</a> <a href="/tags/nosql/" style="font-size: 18.89px;">nosql</a> <a href="/tags/octave/" style="font-size: 10px;">octave</a> <a href="/tags/php/" style="font-size: 20px;">php</a> <a href="/tags/php源码学习/" style="font-size: 17.78px;">php源码学习</a> <a href="/tags/redis/" style="font-size: 12.22px;">redis</a> <a href="/tags/spark/" style="font-size: 12.22px;">spark</a> <a href="/tags/tcp/" style="font-size: 10px;">tcp</a> <a href="/tags/thrift/" style="font-size: 13.33px;">thrift</a> <a href="/tags/分布式/" style="font-size: 10px;">分布式</a> <a href="/tags/基础知识/" style="font-size: 15.56px;">基础知识</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/11/16/Spark-DataSource-Hive Tables/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/11/14/分布式-事务/">分布式-事务</a>
          </li>
        
          <li>
            <a href="/2019/04/29/golang继承/">golang继承</a>
          </li>
        
          <li>
            <a href="/2019/04/28/spark-Quick-Start/">spark quick start</a>
          </li>
        
          <li>
            <a href="/2019/03/20/golang-包导入/">golang包导入</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 Keything<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>